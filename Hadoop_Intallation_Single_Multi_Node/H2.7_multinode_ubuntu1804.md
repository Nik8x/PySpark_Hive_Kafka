Install java
	sudo apt install openjdk-8-jre-headless

find the ip address of both master and slave
	ifconfig
if command dosent work, install net tools
	sudo apt install net-tools
and then check their ip addresses
	ifconfig

For the easy access to the machines have a host name for the each machine in the etc hosts file.
	sudo nano /etc/hosts

    <ip> <name>
    10.20.3.1 master
    10.20.3.2 node1

Let's try to check whether slave can listen to master by pinging it, in master do
	ping node1

Add a hadoop user in both machines.
	sudo adduser hadoop

Login to the master node as hadoop user and generate ssh key pairs.
	ssh-keygen -t rsa

Only make ssh keys in master not in slave. But if you made by mistake on slave, it will be saved as identification and public key, remove them with these commands
	rm /home/hadoop/.ssh/id_rsa.pub
	rm home/hadoop/.ssh/id_rsa

Copy the public key to all data nodes and also to master node if you want use that as a datanode.
	ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@master
	ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node1

if you get the error
ssh: connect t host slave-1 port 22: Connection refused

I'd generally take "connection refused" to mean that the target server isn't listening on port 22.

What's the output of 
	netstat -an|grep 22 
when run on slave-1

it should show 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN

if not then do on Ubuntu user
	sudo apt-get install openssh-server openssh-client
Sometime the error is simple, ssh is not installed. the openssh-client should be installed on your local machine, and openssh-server should be installed on the remote computer. However, you can install both openssh-client and openssh-server on both machines


From Apache hadoop website Download Hadoop binaries not source in ubuntu user

	wget http://mirror.olnevhost.net/pub/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz

NOTE: There is a possibility of below-mentioned error in this setup and installation process.

"hadoop is not in the sudoers file. This incident will be reported." 
This error can be resolved by Login as a root user 
su ubuntu
then write
sudo adduser hadoop sudo


    sudo wget http://mirror.olnevhost.net/pub/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz
    sudo tar -xvf hadoop-2.7.7.tar.gz
    sudo mv hadoop-2.7.7.tar.gz hadoop

Login to hadoop user
Add hadoop path to ~/.bashrc  or ~/.profile file.

    PATH=/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:$PATH

Setting up the master node

We can modify the configuration files on master node and replicate them on other machines.

#Run this to confirm java version
update-alternatives --display java
#Make sure the path is /usr/lib/jvm/java-8-openjdk-amd64/jre


Setup JAVA_HOME environment varibale in hadoop/etc/hadoop/hadoop-env.sh. 

    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/

   source ~/hadoop/etc/hadoop/hadoop-env.sh 

Edit the hadoop/etc/hadoop/core-site.xml. It should contain following xml code.

    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master</value> 
        <description>NameNode URI</description>
      </property>
    </configuration>

Change the hadoop/etc/hadoop/hdfs-site.xml

    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/home/hadoop_cluster_data/temp</value>
        </property>
        <property>
            <name>dfs.name.dir</name>
            <value>/home/hadoop_cluster_data/name</value>
        </property>
            <property>
            <name>dfs.data.dir</name>
            <value>/home/hadoop_cluster_data/data</value>
        </property>
    </configuration>

Rename the mapred-site-template.xml to mapred-site.xml in hadoop/etc/hadoop directory
Add this to hadoop/etc/hadoop/mapred-site-template.xml

    <configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
    </configuration>

Add this to hadoop/etc/hadoop/yarn-site.xml

    <configuration>
        <property>
                <name>yarn.acl.enable</name>
                <value>0</value>
        </property>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>node-master</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
    </configuration>

Edit hadoop/etc/hadoop/slaves

    master
    node1

Adding master also as slave because we are that also as data node.

Now, the configuration for hadoop is done.

Letâ€™s copy whole configuration to other machine.

    cd ~/hadoop
    scp -r $(pwd) hadoop@node1:~/hadoop

write in ubuntu user
hadoop version

if it shows the version then path is set properly, if not do
nano ~/.bashrc
export HADOOP_HOME="/home/ubuntu/Downloads/hadoop"
export PATH="$PATH:$HADOOP_HOME/bin"
source ~/.bashrc

do the same in slave node

Running HDFS

Run following to the format name node.

    hdfs namenode -format

Our hadoop configuration is ready to run. Start the dfs by running following command.

    start-dfs.sh
